{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ab9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Module Requirements:\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential ## !! The Model being used\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64de1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Define Problem\n",
    "#########################\n",
    "## Problem: Build a next word predictor\n",
    "## Goal: Predict the next word in a sequence\n",
    "#####################################################\n",
    "## Outputs for Measuring Quality of Model Validation:\n",
    "### TRUE Positive: Validation Data Label = TRUE;  Machine Learning Label Output = TRUE \n",
    "## FALSE Positive: Validation Data Label = FALSE; Machine Learning Label Output = TRUE \n",
    "### TRUE Negative: Validation Data Label = FALSE; Machine Learning Label Output = FALSE \n",
    "## FALSE Negative: Validation Data Label = TRUE;  Machine Learning Label Output = FALSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7b9a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theory', 'and', 'practice', 'of', 'piano', 'construction', 'by', 'william', 'b', 'white', '--', 'a', 'project', 'gutenberg', 'ebook', 'the', 'project', 'gutenberg', 'ebook', 'of', 'theory', 'and', 'practice', 'of', 'piano', 'construction', 'by', 'william', 'b', 'white', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restriction', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'term', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorglicense', 'title', 'theory', 'and', 'practice', 'of', 'piano', 'construction', 'with', 'a', 'detailed', 'practical', 'method', 'for', 'tuning', 'author', 'william', 'b', 'white', 'release', 'date', 'june', 'ebook', 'language', 'english', 'character', 'set']\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Collect & Split Dataset\n",
    "##################################\n",
    "import text_preprocessor # Import Variables via Files from Directory\n",
    "from text_preprocessor import training_text # Import Variables via Files from Directory\n",
    "##################################\n",
    "## Example text from Training Data\n",
    "print(training_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556a1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theory': 1, 'and': 2, 'practice': 3, 'of': 4, 'piano': 5, 'construction': 6, 'by': 7, 'william': 8, 'b': 9, 'white': 10, '--': 11, 'a': 12, 'project': 13, 'gutenberg': 14, 'ebook': 15, 'the': 16, 'this': 17, 'is': 18, 'for': 19, 'use': 20, 'anyone': 21, 'anywhere': 22, 'at': 23, 'no': 24, 'cost': 25, 'with': 26, 'almost': 27, 'restriction': 28, 'whatsoever': 29, 'you': 30}\n"
     ]
    }
   ],
   "source": [
    "## Get {keyword:id} Pairs\n",
    "keyword_id_dict = {}\n",
    "keyword_id = 1\n",
    "for keyword in range(len(training_text)):\n",
    "    if training_text[keyword] not in keyword_id_dict:\n",
    "        keyword_id_dict[training_text[keyword]] = keyword_id\n",
    "        keyword_id += 1\n",
    "\n",
    "print(dict(itertools.islice(keyword_id_dict.items(), 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eece38da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['theory', 'and', 'practice', 'of'], ['and', 'practice', 'of', 'piano'], ['practice', 'of', 'piano', 'construction'], ['of', 'piano', 'construction', 'by'], ['piano', 'construction', 'by', 'william'], ['construction', 'by', 'william', 'b'], ['by', 'william', 'b', 'white'], ['william', 'b', 'white', '--'], ['b', 'white', '--', 'a'], ['white', '--', 'a', 'project'], ['--', 'a', 'project', 'gutenberg'], ['a', 'project', 'gutenberg', 'ebook'], ['project', 'gutenberg', 'ebook', 'the'], ['gutenberg', 'ebook', 'the', 'project'], ['ebook', 'the', 'project', 'gutenberg'], ['the', 'project', 'gutenberg', 'ebook'], ['project', 'gutenberg', 'ebook', 'of'], ['gutenberg', 'ebook', 'of', 'theory'], ['ebook', 'of', 'theory', 'and'], ['of', 'theory', 'and', 'practice'], ['theory', 'and', 'practice', 'of'], ['and', 'practice', 'of', 'piano'], ['practice', 'of', 'piano', 'construction'], ['of', 'piano', 'construction', 'by'], ['piano', 'construction', 'by', 'william'], ['construction', 'by', 'william', 'b'], ['by', 'william', 'b', 'white'], ['william', 'b', 'white', 'this'], ['b', 'white', 'this', 'ebook'], ['white', 'this', 'ebook', 'is']]\n"
     ]
    }
   ],
   "source": [
    "## Get a list of Text Sequences\n",
    "text_sequences = []\n",
    "sequence_training_len = 4\n",
    "for keyword in range(sequence_training_len, len(training_text)):\n",
    "    sequence = training_text[keyword - sequence_training_len:keyword]\n",
    "    text_sequences.append(sequence)\n",
    "\n",
    "print(text_sequences[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4684b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[453, 4, 152, 2], [4, 152, 2, 168], [152, 2, 168, 77], [2, 168, 77, 14], [168, 77, 14, 992], [77, 14, 992, 121], [14, 992, 121, 746], [992, 121, 746, 162], [121, 746, 162, 5], [746, 162, 5, 80], [162, 5, 80, 273], [5, 80, 273, 644], [80, 273, 644, 1], [273, 644, 1, 80], [644, 1, 80, 273], [1, 80, 273, 644], [80, 273, 644, 2], [273, 644, 2, 453], [644, 2, 453, 4], [2, 453, 4, 152], [453, 4, 152, 2], [4, 152, 2, 168], [152, 2, 168, 77], [2, 168, 77, 14], [168, 77, 14, 992], [77, 14, 992, 121], [14, 992, 121, 746], [992, 121, 746, 12], [121, 746, 12, 644], [746, 12, 644, 6], [12, 644, 6, 16], [644, 6, 16, 1], [6, 16, 1, 163], [16, 1, 163, 2], [1, 163, 2, 747], [163, 2, 747, 1452], [2, 747, 1452, 24], [747, 1452, 24, 65], [1452, 24, 65, 748], [24, 65, 748, 4], [65, 748, 4, 15], [748, 4, 15, 645], [4, 15, 645, 65], [15, 645, 65, 1733], [645, 65, 1733, 2098], [65, 1733, 2098, 111], [1733, 2098, 111, 29], [2098, 111, 29, 432], [111, 29, 432, 9], [29, 432, 9, 122]]\n"
     ]
    }
   ],
   "source": [
    "## Convert list of Text Sequences into its numerical keyword,id Pair\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences) # Updates internal vocabulary based on a list of texts.\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) # Transforms each text in texts to a sequence of integers.\n",
    "print(sequences[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d700a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[          0           0    23689536           1]\n",
      " [     999081           0          -1          -1]\n",
      " [ 2002475924  2123511579  -394174497  1218405217]\n",
      " ...\n",
      " [-1711472021 -1339695325  -309812460 -1236975977]\n",
      " [ 1449989897  1643624775 -1309731435 -1063851751]\n",
      " [ 1099052232  -884821824  1580753708   858819792]]\n"
     ]
    }
   ],
   "source": [
    "## Get a list of empty sequences\n",
    "n_sequences = np.empty([len(sequences), sequence_training_len], dtype='int32')\n",
    "print(n_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab240d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep the training data\n",
    "for sequence in range(len(sequences)):\n",
    "    n_sequences[sequence] = sequences[sequence]\n",
    "train_inputs = n_sequences[:,:-1] # Gets every keyword in a sequence except the last one (the keywords leading up to the TARGET)\n",
    "train_targets = n_sequences[:,-1] # Gets the last keyword in a sequence (the TARGET)\n",
    "vocabulary_size = len(tokenizer.word_counts) + 1 # vocabulary_size increased by 1 because of Padding\n",
    "train_targets = to_categorical(train_targets, num_classes = vocabulary_size) # Converts a class vector (integers) to binary class matrix\n",
    "sequence_len = train_inputs.shape[1] # The number of keywords in a sequence from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b14e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Train Model\n",
    "#################\n",
    "## Prep the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, sequence_len, input_length=sequence_len))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e5aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1918/1918 [==============================] - 20s 8ms/step - loss: 6.3515 - accuracy: 0.1025 - mse: 2.0811e-04\n",
      "Epoch 2/50\n",
      "1918/1918 [==============================] - 16s 8ms/step - loss: 6.0526 - accuracy: 0.1057 - mse: 2.0649e-04\n",
      "Epoch 3/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 5.8428 - accuracy: 0.1331 - mse: 2.0294e-04\n",
      "Epoch 4/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 5.6710 - accuracy: 0.1401 - mse: 2.0160e-04\n",
      "Epoch 5/50\n",
      "1918/1918 [==============================] - 16s 8ms/step - loss: 5.5384 - accuracy: 0.1435 - mse: 2.0025e-04\n",
      "Epoch 6/50\n",
      "1918/1918 [==============================] - 17s 9ms/step - loss: 5.4253 - accuracy: 0.1480 - mse: 1.9881e-04\n",
      "Epoch 7/50\n",
      "1918/1918 [==============================] - 16s 8ms/step - loss: 5.3133 - accuracy: 0.1562 - mse: 1.9764e-04\n",
      "Epoch 8/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 5.1918 - accuracy: 0.1674 - mse: 1.9641e-04\n",
      "Epoch 9/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 5.0712 - accuracy: 0.1743 - mse: 1.9531e-04\n",
      "Epoch 10/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 4.9656 - accuracy: 0.1776 - mse: 1.9443e-04\n",
      "Epoch 11/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 4.8743 - accuracy: 0.1817 - mse: 1.9375e-04\n",
      "Epoch 12/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.7911 - accuracy: 0.1859 - mse: 1.9305e-04\n",
      "Epoch 13/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 4.7105 - accuracy: 0.1904 - mse: 1.9247e-04\n",
      "Epoch 14/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.6358 - accuracy: 0.1944 - mse: 1.9191e-04\n",
      "Epoch 15/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 4.5672 - accuracy: 0.1976 - mse: 1.9149e-04\n",
      "Epoch 16/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.5019 - accuracy: 0.2005 - mse: 1.9103e-04\n",
      "Epoch 17/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 4.4412 - accuracy: 0.2047 - mse: 1.9063e-04\n",
      "Epoch 18/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.3843 - accuracy: 0.2042 - mse: 1.9030e-04\n",
      "Epoch 19/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.3321 - accuracy: 0.2083 - mse: 1.8997e-04\n",
      "Epoch 20/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.2851 - accuracy: 0.2102 - mse: 1.8961e-04\n",
      "Epoch 21/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 4.2406 - accuracy: 0.2120 - mse: 1.8933e-04\n",
      "Epoch 22/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.1994 - accuracy: 0.2138 - mse: 1.8903e-04\n",
      "Epoch 23/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 4.1620 - accuracy: 0.2179 - mse: 1.8870e-04\n",
      "Epoch 24/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 4.1249 - accuracy: 0.2193 - mse: 1.8838e-04\n",
      "Epoch 25/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.0912 - accuracy: 0.2216 - mse: 1.8813e-04\n",
      "Epoch 26/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 4.0611 - accuracy: 0.2238 - mse: 1.8775e-04\n",
      "Epoch 27/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 4.0318 - accuracy: 0.2274 - mse: 1.8743e-04\n",
      "Epoch 28/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 4.0013 - accuracy: 0.2289 - mse: 1.8716e-04\n",
      "Epoch 29/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 3.9737 - accuracy: 0.2310 - mse: 1.8682e-04\n",
      "Epoch 30/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.9485 - accuracy: 0.2348 - mse: 1.8655e-04\n",
      "Epoch 31/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.9223 - accuracy: 0.2354 - mse: 1.8624e-04\n",
      "Epoch 32/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.8981 - accuracy: 0.2382 - mse: 1.8602e-04\n",
      "Epoch 33/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.8738 - accuracy: 0.2395 - mse: 1.8567e-04\n",
      "Epoch 34/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.8544 - accuracy: 0.2411 - mse: 1.8538e-04\n",
      "Epoch 35/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.8321 - accuracy: 0.2427 - mse: 1.8511e-04\n",
      "Epoch 36/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.8136 - accuracy: 0.2441 - mse: 1.8487e-04\n",
      "Epoch 37/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 3.7938 - accuracy: 0.2472 - mse: 1.8449e-04\n",
      "Epoch 38/50\n",
      "1918/1918 [==============================] - 17s 9ms/step - loss: 3.7746 - accuracy: 0.2481 - mse: 1.8425e-04\n",
      "Epoch 39/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.7590 - accuracy: 0.2496 - mse: 1.8401e-04\n",
      "Epoch 40/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.7427 - accuracy: 0.2525 - mse: 1.8373e-04\n",
      "Epoch 41/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.7252 - accuracy: 0.2530 - mse: 1.8350e-04\n",
      "Epoch 42/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 3.7096 - accuracy: 0.2561 - mse: 1.8321e-04\n",
      "Epoch 43/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.6955 - accuracy: 0.2572 - mse: 1.8291e-04\n",
      "Epoch 44/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.6808 - accuracy: 0.2585 - mse: 1.8271e-04\n",
      "Epoch 45/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.6661 - accuracy: 0.2593 - mse: 1.8250e-04\n",
      "Epoch 46/50\n",
      "1918/1918 [==============================] - 14s 8ms/step - loss: 3.6537 - accuracy: 0.2608 - mse: 1.8224e-04\n",
      "Epoch 47/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.6383 - accuracy: 0.2618 - mse: 1.8197e-04\n",
      "Epoch 48/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.6267 - accuracy: 0.2640 - mse: 1.8181e-04\n",
      "Epoch 49/50\n",
      "1918/1918 [==============================] - 15s 8ms/step - loss: 3.6136 - accuracy: 0.2656 - mse: 1.8152e-04\n",
      "Epoch 50/50\n",
      "1918/1918 [==============================] - 14s 7ms/step - loss: 3.6013 - accuracy: 0.2665 - mse: 1.8133e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff081acd730>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse']) # Compile defines the loss function, the optimizer and the metrics\n",
    "model.fit(train_inputs, train_targets, epochs=50, verbose=1) ## Fit is used for training the model with the provided inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698019e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 words in a sequence:\n",
      "if the tuned\n",
      "\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Suggested next word: 'prop'\n",
      "Suggested next word: 'describes'\n",
      "Suggested next word: 'boring'\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Debug & Tune Model\n",
    "## Debug & Tune Model: Validate Model using the Debugging Dataset\n",
    "#### Review Machine Learning Label Output vs Debugging Dataset Label\n",
    "#### IF inspired THEN fix issues (dataset, hyperparameters, etc.)\n",
    "validation_text = 'if the tuned'\n",
    "validation_text = validation_text.strip().lower()\n",
    "print('First 3 words in a sequence:')\n",
    "print(validation_text)\n",
    "print('\\n')\n",
    "\n",
    "encoded_text = tokenizer.texts_to_sequences([validation_text])[0] ## Converts validation text to keyword_id\n",
    "pad_encoded = pad_sequences([encoded_text], maxlen=sequence_len, truncating='pre') ## Converts keyword_ids to array\n",
    "\n",
    "for keyword in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "    predicted_next_word = tokenizer.index_word[keyword]\n",
    "    print(\"Suggested next word: '{0}'\".format(predicted_next_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01598c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1d904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a1744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef5d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2af003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89488b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
