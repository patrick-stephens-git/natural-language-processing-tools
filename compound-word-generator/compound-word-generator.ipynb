{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4210331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_compound_phrases_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37ea4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import urllib.request, json\n",
    "import urllib.parse # For encoding the URL string to UTF-8\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import en_core_web_sm\n",
    "import contractions\n",
    "import unidecode\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Don't collapse Pandas Dataframes:\n",
    "pd.set_option('display.max_rows', None, 'display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a167d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import Training Data (Text Corpus) via input URL:\n",
    "# url = 'https://en.wikipedia.org/wiki/Music_theory'\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n",
    "#           }\n",
    "# req = requests.get(url, headers)\n",
    "# soup = BeautifulSoup(req.content, 'html.parser')\n",
    "# training_text = soup.get_text()\n",
    "# print(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc66187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  `Anyone who has not worked for them\n",
      "     simply cannot understand them.'\n",
      "\n",
      "         - Mille Vennamun, introduction to:\n",
      "          `The Use of Ashes: Bureau of Procuration Manual'\n",
      "\n",
      "  Half past eight.  The bedside alarm woke Kelanie up with the\n",
      "sampled victory-screech of some carnivorous xenoform.  She was up\n",
      "immediately, eyes wide, fingers clawing the pillow-pads, gasping with\n",
      "shock as the subconsciously-induced adrenalin shivered through her\n",
      "system.   As she calmed down, her pupils dilated out from crisis-\n",
      "induced pinpricks, her breathing and pulse rates returned to normal,\n",
      "and she wondered, not for the first or last time, if life was like\n",
      "this in the private sector.   She scrambled off the bed as it began\n",
      "to deflate and retract into the wall.\n",
      "  Her personalised holographic news service activated as she stepped\n",
      "into the shower.   It took the appearance of an old man dressed in a\n",
      "monk's habit, who bore a strong resemblance to William S. Burroughs.\n",
      "It leered at her, and croaked,\n",
      "  `Rough night last night?'   She pushed the oxygen control with the\n",
      "heel of her hand, took a few deep snorts.   Under the stream of\n",
      "high-pressure hot water, she soaped herself and replied,\n",
      "  `Mind your own ratty business, line-noise.   What's on the agenda\n",
      "for today?'   The news laughed, wheezing and rasping.\n",
      "  `Come on, seriously!   I refuse to believe that you don't remember\n",
      "the event you have been awaiting, for - how long has it been?'   She\n",
      "turned off the shower, snorted some more oxygen and, with a warm\n",
      "towel over her shoulders, found some clean underwear and a long\n",
      "jumper she had only worn three times since it was washed last.\n",
      "  `Three months.   You can assume that I've been on the ExPort\n",
      "waiting list for so long that I've forgotten where I'm supposed to be\n",
      "going.   Put on some music and refresh my memory.'   The news spoke\n",
      "over the soft sounds of a song by `This Mortal Coil':\n",
      "  `You are due at the NoSan'No'Os ExPort at nine-thirty, to check in\n",
      "for your pre-flight examination and briefi\n"
     ]
    }
   ],
   "source": [
    "## Import Training Data (Text Corpus) via input File:\n",
    "with open('training-text.txt', encoding='utf-8') as input_file:\n",
    "    training_text = input_file.read()\n",
    "sample_training_text = training_text[0:2000]\n",
    "print(sample_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb56a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_list = []\n",
    "training_text_list.append(training_text)\n",
    "# training_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce9a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "## Clean Training Data: (Text Pre-processing)\n",
    "def text_preprocessor(text):\n",
    "    ###########################################\n",
    "    soup = BeautifulSoup(text, 'html.parser') ## Remove HTML\n",
    "    text = soup.get_text(separator=' ') ## Remove HTML\n",
    "    ###########################################\n",
    "    text = text.lower() ## Lowercase Characters\n",
    "    text = contractions.fix(text) ## Expand Contractions (\"don't\" -> \"do not\")\n",
    "    text = re.sub(r'https?:\\S*', '', text) ## Remove URLs\n",
    "    text = re.sub(r'@\\S*', '', text) ## Remove Twitter Mentions\n",
    "    text = re.sub(r'#\\S*', '', text) ## Remove Hashtags\n",
    "    text = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', text) ## Remove special characters (e.g: %, $, &, etc.)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) ## Remove Punctuation Characters\n",
    "    text = re.sub(r'[0-9]+', '', text) ## Remove Numerical Characters\n",
    "    text = unidecode.unidecode(text) ## Normalized accented characters (ñ -> n)\n",
    "    ###########################################\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    ###########################################\n",
    "    text = word_tokenize(text) ## Tokenize Text    \n",
    "    stop_words = set(stopwords.words('english')) ## Get Stop Words\n",
    "    stop_words_exclusion = ['no','not','nor'] ## Stop Word Exclusion List\n",
    "    stop_words = [word for word in stop_words if word not in stop_words_exclusion] ## Remove Stop Word Exclusions from Stop Words\n",
    "    text = [word for word in text if word not in stop_words] ## Remove Stop Words\n",
    "    ###########################################\n",
    "    # ps = PorterStemmer() ## Stemming: ['wait', 'waiting', 'waited', 'waits'] -> 'wait'\n",
    "    # text = [ps.stem(word) for word in text] ## Apply Word Stemming\n",
    "    wnl = WordNetLemmatizer() ## Lemmatization: 'studies' -> 'study'; 'studying' -> 'studying'\n",
    "    text = [wnl.lemmatize(word) for word in text] ## Apply Word Lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1082038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_super_df = pd.DataFrame()\n",
    "\n",
    "for training_text in training_text_list:\n",
    "    text = training_text\n",
    "    \n",
    "    nlp = en_core_web_sm.load() \n",
    "    doc = nlp(text[:900000]) ###<<<<<<<<<<<<<<###<<<<<<<<<<<<<<###<<<<<<<<<<<<<<###<<<<<<<<<<<<<<###<<<<<<<<<<<<<<\n",
    "    array = [(X, X.ent_iob_, X.ent_type_) for X in doc]\n",
    "    df = pd.DataFrame(array, columns=['word','iob','entity type'])\n",
    "    description_dict = {'B':'beginning of an entity',\n",
    "                        'I':'inside of an entity',\n",
    "                        'O':'outside of an entity'}\n",
    "    df['iob description'] = df['iob'].map(description_dict)\n",
    "    description_dict = {'PERSON':'people, including fictional',\n",
    "                        'NORP':'nationalities, religious or political groups',\n",
    "                        'FAC':'buildings, airports, highways, bridges, etc',\n",
    "                        'ORG':'companies, agencies, institutions, etc',\n",
    "                        'GPE':'countries, cities, states',\n",
    "                        'LOC':'non-GPE locations, mountain ranges, bodies of water',\n",
    "                        'PRODUCT':'objects, vehicles, foods, etc (not services)',\n",
    "                        'EVENT':'named hurricanes, battles, wars, sports events, etc',\n",
    "                        'WORK_OF_ART':'titles of books, songs, etc',\n",
    "                        'LAW':'named documents made into laws',\n",
    "                        'LANGUAGE':'any named language',\n",
    "                        'DATE':'absolute or relative dates or periods',\n",
    "                        'TIME':'times smaller than a day',\n",
    "                        'PERCENT':'percentage',\n",
    "                        'MONEY':'monetary values',\n",
    "                        'QUANTITY':'measurements, as of weight or distances',\n",
    "                        'ORDINAL':'first, second, etc',\n",
    "                        'CARDINAL':'numerals that do not fall under another type'\n",
    "                       }\n",
    "    df['entity type description'] = df['entity type'].map(description_dict)\n",
    "    df = df[df['iob'].str.contains('O') == False]\n",
    "    # df.head(100)\n",
    "    \n",
    "    entity_type_list = []\n",
    "    compound_phrase_list = []\n",
    "    iob_compound_list = []\n",
    "    prev_iob = 'B'\n",
    "    for index, row in df.iterrows():\n",
    "        curr_iob = row['iob']\n",
    "        word = row['word']\n",
    "        entity_type = row['entity type']\n",
    "        # print(iob)\n",
    "        if prev_iob == 'B' and curr_iob == 'B':\n",
    "            prev_iob = curr_iob\n",
    "            try:\n",
    "                list_of_strings = [i.text for i in iob_compound_list]\n",
    "                # print(list_of_strings)\n",
    "                list_of_compound_phrases = ' '.join(list_of_strings)\n",
    "            except:\n",
    "                pass\n",
    "            compound_phrase_list.append(list_of_compound_phrases)\n",
    "            entity_type_list.append(entity_type)\n",
    "            iob_compound_list = []\n",
    "            iob_compound_list.append(word)\n",
    "            # print(iob_compound_list)\n",
    "        elif prev_iob == 'B' and curr_iob == 'I':\n",
    "            prev_iob = curr_iob\n",
    "            iob_compound_list.append(word)\n",
    "            # print(iob_compound_list)\n",
    "        elif prev_iob == 'I' and curr_iob == 'I':\n",
    "            prev_iob = curr_iob\n",
    "            iob_compound_list.append(word)\n",
    "            # print(iob_compound_list)\n",
    "        elif prev_iob == 'I' and curr_iob == 'B':\n",
    "            prev_iob = curr_iob\n",
    "            try:\n",
    "                list_of_strings = [i.text for i in iob_compound_list]\n",
    "                # print(list_of_strings)\n",
    "                list_of_compound_phrases = ' '.join(list_of_strings)\n",
    "            except:\n",
    "                pass\n",
    "            compound_phrase_list.append(list_of_compound_phrases)\n",
    "            entity_type_list.append(entity_type)\n",
    "            iob_compound_list = []\n",
    "            iob_compound_list.append(word)\n",
    "            # print(iob_compound_list)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    compound_phrase_list = compound_phrase_list[1:] # Removes empty value from start of list\n",
    "    compound_phrase_df = pd.DataFrame(compound_phrase_list, columns=['compound phrase'])\n",
    "    compound_phrase_entity_df = pd.DataFrame(entity_type_list, columns=['entity type'])\n",
    "    result_df = pd.concat([compound_phrase_df, compound_phrase_entity_df], axis=1)\n",
    "    # result_df.drop_duplicates().sort_values(by='compound phrase', ascending=True).head(1000)\n",
    "    \n",
    "    compound_phrases_df = pd.DataFrame()\n",
    "    # compound_phrases_df['compound phrase'] = result_df[~result_df['compound phrase'].str.contains('^\\w*$')]\n",
    "    result_df = result_df.dropna() # Removes any NaN values\n",
    "    compound_phrases_df = result_df[~result_df['compound phrase'].str.match('^\\w*$')] # Removes single words from the results to get compound queries\n",
    "\n",
    "    compound_phrases_df = compound_phrases_df['compound phrase'].sort_values(ascending=True)\n",
    "    compound_phrases_df = compound_phrases_df.drop_duplicates()\n",
    "\n",
    "    compound_phrases_list = compound_phrases_df.tolist()\n",
    "    # print(compound_phrases_list)\n",
    "    prepocessed_compound_phrase_list = []\n",
    "    for compound_phrase in compound_phrases_list:\n",
    "        text = compound_phrase\n",
    "        ###########################################\n",
    "        soup = BeautifulSoup(text, 'html.parser') ## Remove HTML\n",
    "        text = soup.get_text(separator=' ') ## Remove HTML\n",
    "        ###########################################\n",
    "        text = text.lower() ## Lowercase Characters\n",
    "        #text = contractions.fix(text) ## Expand Contractions (\"don't\" -> \"do not\")\n",
    "        #text = re.sub(r'https?:\\S*', '', text) ## Remove URLs\n",
    "        #text = re.sub(r'@\\S*', '', text) ## Remove Twitter Mentions\n",
    "        #text = re.sub(r'#\\S*', '', text) ## Remove Hashtags\n",
    "        text = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', text) ## Remove special characters (e.g: %, $, &, etc.)\n",
    "        text = text.replace('\\n','')\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation)) ## Remove Punctuation Characters\n",
    "        # text = re.sub(r'[0-9]+', '', text) ## Remove Numerical Characters\n",
    "        text = unidecode.unidecode(text) ## Normalized accented characters (ñ -> n)\n",
    "        ###########################################\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text) # no emoji\n",
    "        ###########################################\n",
    "        # text = word_tokenize(text) ## Tokenize Text    \n",
    "        # stop_words = set(stopwords.words('english')) ## Get Stop Words\n",
    "        # stop_words_exclusion = ['no','not','nor'] ## Stop Word Exclusion List\n",
    "        # stop_words = [word for word in stop_words if word not in stop_words_exclusion] ## Remove Stop Word Exclusions from Stop Words\n",
    "        # text = [word for word in text if word not in stop_words] ## Remove Stop Words\n",
    "        ###########################################\n",
    "        # ps = PorterStemmer() ## Stemming: ['wait', 'waiting', 'waited', 'waits'] -> 'wait'\n",
    "        # text = [ps.stem(word) for word in text] ## Apply Word Stemming\n",
    "        # wnl = WordNetLemmatizer() ## Lemmatization: 'studies' -> 'study'; 'studying' -> 'studying'\n",
    "        # text = [wnl.lemmatize(word) for word in text] ## Apply Word Lemmatization\n",
    "        ###########################################\n",
    "        text = text.strip()\n",
    "        text = text.replace('  ',' ')\n",
    "        text = text.replace('  ',' ')\n",
    "        prepocessed_compound_phrase_list.append(text)\n",
    "        \n",
    "        new_df = pd.DataFrame(prepocessed_compound_phrase_list)\n",
    "    new_df = new_df[~new_df[0].str.match('^\\w*$')] # Removes single words from the results to get compound queries\n",
    "    prepocessed_compound_phrase_list = new_df[0].tolist()\n",
    "    # print(prepocessed_compound_phrase_list)\n",
    "    super_compound_phrases_list.append(prepocessed_compound_phrase_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7884b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about an hour', 'a bureau of procuration', 'tsiry feylen kendr tariy', 'mileva barker', 'six hundred and fifty one', 'thirty two kilometres', 'rik mayall', 'chapter one the export', 'handelsman tsiry feylen', 'three feet', 'between five and twelve', 'nikolai kingsley', 'one day', 'waddell s emporium of extremely fashionable attire and quite nice ice cream parlour', 'two metre tall', 'more than ninety thousand', 'about fifty', 'the technological interdict', 'millimillenarian technological control', 'two hundred', 'comoncurensy isotope', 'earth diplomatic exchange', 'maracite information exchange registry', 'about seven hundred kilometres', 'the bythenet relay network', 'twelve years old', 'mews emerged', 'undefined undefined haircut', 'earth export', 'eight hundred and ten', 'at least two million miles', 'eight metres', 'one minute', 'terrestrial anglic', 'forty hours', 'seven years', 'these days', 'minus two', 'one hundred percent', 'nine hundred', 'the barber', 'kelanie and marek', 'between three and seven days', 'six months', 'twenty one minutes', 'one foot', 'a few hours later', 'forty degrees', 'only four hours ago', 'thirty four hours', 'three quarters', 'the interdicted moridani', 'robert smith', 'li svayene', 'twenty eight', 'marek waddell', 'up yours ugly', 'this morning', 'finding free data channel please wait', 'some ninety thousand kilometres', 'the past week', 'the interdiction conference zone', 'three thousand years ago', 'marek keery', 'plateau bythians', 'sixteen years ago', 'the underground networks', 'two days later', 'the tendeysharhi embassy', 'three metres', 'alexander pope', 'about two metres', 'the office of threat termination', 'about three meters', 'napaisub 997193', 'half an hour', 'ninety years', 'technological control   bureau', 'preliminary  recommendation', 'a hundred years ago', 'six hundred and ninety three', 'a few moments', '40 hours', 'hundreds of years', 'church of the subgenius', 'forty one', 'every two minutes or so', 'waddell s emporium of extremely fashionable attire', 'pre millennium', 'twenty seven days', 'kayren kayley', 'her days', 'about twelve million miles', 'nine thirty', 'almost two metres', 'moridani cause', 'every thirty seconds', 'aln riker', 'hundreds of thousands', 'three kilometres', 'circle within the circle', 'interspecies advisory', 'a minute', 'some two hours', 'n frf knh k', 'ten to the sixth kilometres', 'moridani   phandric', 'nine hour', 'three weeks ago', '35 minutes 45 seconds', 'seff cafe', 'eleven hundred', 'quite nice ice cream parlour', 'a waddell special', 'one hundred', 'kelanie s', 'six years', 'three days', '978 percent', 'this afternoon', 'six hundred', 'six foot', 'the bythians', 'nine years', 'just under two metres', 'gaeren tuuri', 'her work credit hour meter', 'saint bernard', 'barber xeno', 'the kaelen queen', 'bureau of procuration', 'moridani partisan', 'plateau bythian', 'about a metre', 'eleven fifty', 'some two thousand', 'n svw tre a', 'this end believes', 'tsiry feylen s', 'ninety year', 'the pthalklin ervae', 'seven thousand years', 'six month s', 'tendeysharh embassy staff', 'miss camden', 'chapter five', 'the bythian militia', 'the second expansion', 'eris fnord', '1730 marek', 'baylal delvoy kendr teff', 'last year', 'the use of ashes bureau of procuration manual  half past eight', 'tsiry feylen', 'four years', '195 km sec', 'three or four', 'about a month s', 'only four', 'almost ninety years ago', 'two metres', 'a few minutes', 'ninety two years', 'guidance ai', 'eighty degrees', 'ten minutes', '180 degree', 'at least thirty', 'three centimetre long', 'one season', 'the bureau of procuration', 'the millimillenarian napaisub', 'three months', 'about a dozen', 'triple s', 'some twenty hours later', 'robyn starkey', 'a few metres', 'this mortal coil  you', 'psym motd', 'diplomatic exchange', 'chapter two', 'kendr saranaxio parndta athanasius', 'ninety five', 'martini baton', 'pthalklin ervae', 'n frf bla g', 'five kilometres', 'kendr saranaxio', 'william s burroughs', 'almost three metres']\n"
     ]
    }
   ],
   "source": [
    "flat_super_compound_phrases_list = [item for sublist in super_compound_phrases_list for item in sublist]\n",
    "flat_super_compound_phrases_list = list(set(flat_super_compound_phrases_list))\n",
    "print(flat_super_compound_phrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5886a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1f691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90777936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e57591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
