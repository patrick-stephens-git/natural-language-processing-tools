{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386359bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import unidecode\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c428646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import Training Data (Text Corpus) via input URL:\n",
    "# url = 'https://en.wikipedia.org/wiki/Music_theory'\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Mobile Safari/537.36 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'\n",
    "#           }\n",
    "# req = requests.get(url, headers)\n",
    "# soup = BeautifulSoup(req.content, 'html.parser')\n",
    "# training_text = soup.get_text()\n",
    "# print(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0743176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game is on ðŸ”¥ðŸ”¥\n",
      "\n",
      "Spell out contractions\n",
      "The contractions package in python (which you need to install using !pip install contractions) allows us to spell out contractions. Spelling out contractions can add more information to your text data by letting more tokens to be created when tokenization is performed. For instance, in the code snippet below, the token â€œwouldâ€ is not considered as a separate token when word tokenization based on white space is performed. Instead, it lives as part of the token â€œSheâ€™dâ€. Once we fix the contractions, however, we see that the word â€œwouldâ€ lives as a standalone token when word tokenization is performed. This adds more tokens for the NLP model to make use of. This may help the model better understand what the text means and thereby improve accuracy for various NLP tasks.\n",
      "\n",
      "https://towardsdatascience.com/primer-to-cleaning-text-data-7e856d6e5791\n",
      "\n",
      "Thank you @Jay for your contribution to this project! #projectover\n"
     ]
    }
   ],
   "source": [
    "## Import Training Data (Text Corpus) via input File:\n",
    "with open('training-text.txt', encoding='utf-8') as input_file:\n",
    "    training_text = input_file.read()\n",
    "sample_training_text = training_text[0:2000]\n",
    "print(sample_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24809278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Training Data: (Text Pre-processing)\n",
    "def text_preprocessor(text):\n",
    "    ###########################################\n",
    "    # soup = BeautifulSoup(text, 'html.parser') ## Remove HTML\n",
    "    # text = soup.get_text(separator=' ') ## Remove HTML\n",
    "    ###########################################\n",
    "    text = text.lower() ## Lowercase Characters\n",
    "    text = contractions.fix(text) ## Expand Contractions (\"don't\" -> \"do not\")\n",
    "    text = re.sub(r'https?:\\S*', '', text) ## Remove URLs\n",
    "    text = re.sub(r'@\\S*', '', text) ## Remove Twitter Mentions\n",
    "    text = re.sub(r'#\\S*', '', text) ## Remove Hashtags\n",
    "    text = re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', text) ## Remove special characters (e.g: %, $, &, etc.)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) ## Remove Punctuation Characters\n",
    "    text = re.sub(r'[0-9]+', '', text) ## Remove Numerical Characters\n",
    "    text = unidecode.unidecode(text) ## Normalized accented characters (Ã± -> n)\n",
    "    ###########################################\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    ###########################################\n",
    "    text = word_tokenize(text) ## Tokenize Text    \n",
    "    stop_words = set(stopwords.words('english')) ## Get Stop Words\n",
    "    stop_words_exclusion = ['no','not','nor'] ## Stop Word Exclusion List\n",
    "    stop_words = [word for word in stop_words if word not in stop_words_exclusion] ## Remove Stop Word Exclusions from Stop Words\n",
    "    text = [word for word in text if word not in stop_words] ## Remove Stop Words\n",
    "    ###########################################\n",
    "    # ps = PorterStemmer() ## Stemming: ['wait', 'waiting', 'waited', 'waits'] -> 'wait'\n",
    "    # text = [ps.stem(word) for word in text] ## Apply Word Stemming\n",
    "    wnl = WordNetLemmatizer() ## Lemmatization: 'studies' -> 'study'; 'studying' -> 'studying'\n",
    "    text = [wnl.lemmatize(word) for word in text] ## Apply Word Lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e707de16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'spell', 'contraction', 'contraction', 'package', 'python', 'need', 'install', 'using', 'pip', 'install', 'contraction', 'allows', 'u', 'spell', 'contraction', 'spelling', 'contraction', 'add', 'information', 'text', 'data', 'letting', 'token', 'created', 'tokenization', 'performed', 'instance', 'code', 'snippet', 'token', 'would', 'not', 'considered', 'separate', 'token', 'word', 'tokenization', 'based', 'white', 'space', 'performed', 'instead', 'life', 'part', 'token', 'would', 'fix', 'contraction', 'however', 'see', 'word', 'would', 'life', 'standalone', 'token', 'word', 'tokenization', 'performed', 'add', 'token', 'nlp', 'model', 'make', 'use', 'may', 'help', 'model', 'better', 'understand', 'text', 'mean', 'thereby', 'improve', 'accuracy', 'various', 'nlp', 'task', 'thank', 'contribution', 'project']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_training_text = text_preprocessor(training_text)\n",
    "print(preprocessed_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12ebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80f247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bd69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e64662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c92a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023d880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
